\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{float}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[margin=1in]{geometry}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue
}

% Code listing style (if needed)
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny,
    showstringspaces=false
}

\title{\textbf{Exercise 1: Classification\\
VU Machine Learning 2025W}}

\author{
    Group 68\\[0.3cm]
    Campbell Lucas (12536848)\\
    Kohl Leonhard (12047036)\\
    Scherling Christopher (12119060)
}

\date{\today}

\begin{document}

\maketitle
\thispagestyle{empty}
\newpage

\tableofcontents
\newpage

\setcounter{page}{1}

%=============================================================================
\section{Introduction}
\label{sec:introduction}

% Brief overview of the exercise objectives
% What you aim to investigate
% Structure of the report

This report presents our work on Exercise 1 of the Machine Learning course, focusing on classification tasks across multiple datasets using different algorithms. The main objectives are to experiment with various classifier-dataset combinations, analyze preprocessing strategies, evaluate performance using multiple metrics, and compare holdout and cross-validation approaches.


%=============================================================================
\section{Datasets}
\label{sec:datasets}

% Describe each of the 4 datasets
% 2 from Kaggle competition
% 2 from Exercise 0
% Justify your choice - explain diversity:
%   - number of samples (small vs large)
%   - number of dimensions (low vs high)
%   - number of classes (few vs many)
%   - preprocessing needs

We selected four diverse datasets to comprehensively evaluate classifier performance across different data characteristics:

\subsection{Dataset 1: [Voting Records]}
\label{subsec:dataset1}

\textbf{Source:} Kaggle Competition (Course-specific)

\textbf{Characteristics:}
\begin{itemize}
    \item Number of samples (train/test): 218 / 217
    \item Number of features: 16
    \item Number of classes: 2 (Democrat vs. Republican)
    \item Feature types: Categorical (voting records on various congressional bills)
    \item Class distribution: Imbalanced (58.3\% Democrat / 41.7\% Republican)
\end{itemize}

\textbf{Description:}
This dataset contains congressional voting records for U.S. House Representatives, with each feature representing a vote (yes/no/abstain) on specific legislation. The classification task is to predict party affiliation (Democrat or Republican) based on voting patterns.

\textbf{Challenges:}
This dataset had a small sample size of only 218 training samples which created a big risk of overfitting. It also had a slight class imbalance which could lead to a prediction bias towards the majority class. Also missing votes needed appropriate handling.

\subsection{Dataset 2: [Amazon Product Reviews]}
\label{subsec:dataset2}

\textbf{Source:} Kaggle Competition (Course-specific)

\textbf{Characteristics:}
\begin{itemize}
    \item Number of samples (train/test): 750 / 750
    \item Number of features: 10,000
    \item Number of classes: 50 (product categories)
    \item Feature types: Numerical
    \item Class distribution: Balanced (~2\% per class)
\end{itemize}

\textbf{Description:}
This text classification dataset contains Amazon product reviews that have been preprocessed into a high-dimensional feature representation. Each review must be classified into one of 50 categories, each class being the name of a reviewer.

\textbf{Challenges:}
This dataset had a very high dimensionality which was challenging. It also had a relatively small sample size compared to the feature count.


\subsection{Dataset 3: [Phishing Website Detection]}
\label{subsec:dataset3}

\textbf{Source:} Exercise 0

\textbf{Characteristics:}
\begin{itemize}
    \item Number of samples: 1,353
    \item Number of features: 9
    \item Number of classes: 3 (phishing status: -1, 0, 1)
    \item Feature types: Categorical (website characteristics)
    \item Class distribution: Imbalanced (51.9\% / 40.5\% / 7.6\%)
\end{itemize}

\textbf{Description:}
This dataset contains features describing website characteristics used to detect phishing attempts. 
Features include URL structure, domain age, SSL certificate status, and other security-related attributes. 
The three classes represent different levels of phishing risk or legitimacy. 

\textbf{Challenges:}
Since all features were categorical proper encoding was needed. We also had a slight class imbalance in the dataset (7.6\%).
Also the 3-class problem is a bit more complex compared to binary classification. There was also some collinearity present among the predictors.


\subsection{Dataset 4: [Road Safety Casualties]}
\label{subsec:dataset4}

\textbf{Source:} Exercise 0

\textbf{Characteristics:}
\begin{itemize}
    \item Number of samples: 363,243 (subsampled to 10,000 for computational feasibility)
    \item Number of features: 66 (59 categorical, 7 numerical)
    \item Number of classes: 11 (casualty severity levels)
    \item Feature types: Mixed (predominantly categorical)
    \item Class distribution: Imbalanced (largest class 19.2\%, smallest class 5.6\%)
\end{itemize}

\textbf{Description:}
This large-scale dataset from the UK Department for Transport contains detailed records of traffic accidents, including information about road conditions, weather, vehicle types, location characteristics, and casualty outcomes. 
The classification task is to predict Age\_Band\_of\_Driver (different age ranges, rather than predicting numerical value of specific age) based on accident circumstances.
This represents a real-world public safety application where experiment outcomes could help inform policy decisions and safety measures.

\textbf{Challenges:}
The dataset is massive with 363.000 samples which made it really challenging to compute. Some teammembers needed to use subsampling since we didn't have the needed compute power.
Also since 11 classes are present this creates very complex decision boundaries. There are also missing values across multiple features. 


%=============================================================================
\section{Data Preprocessing}
\label{sec:preprocessing}

% Describe preprocessing steps for each dataset
% Justify why you chose these steps
% Discuss impact of preprocessing (especially scaling) on results

This section describes the preprocessing steps applied to each dataset and justifies these choices based on data characteristics and classifier requirements.

\subsection{General Preprocessing Pipeline}

Our preprocessing pipeline follows these general steps:
\begin{enumerate}
    \item Data loading and initial exploration
    \item Handling missing values
    \item Outlier detection and treatment
    \item Feature encoding (for categorical variables)
    \item Feature scaling/normalization
    \item Feature selection (if applicable)
\end{enumerate}

\subsection{Dataset-Specific Preprocessing}

\subsubsection{Voting Records Dataset Preprocessing}

\textbf{Missing Values:} 
The voting dataset contained missing values represented as "unknown" in the original data. We adopted a preservation strategy, converting "unknown" to NaN. This approach was chosen because:
\begin{itemize}
    \item Voting abstentions can themselves be datapoints, if the model can use them
    \item Many scikit-learn classifiers (Decision Trees, KNN) can handle NaN values natively
    \item Imputation could introduce artificial patterns that don't represent actual voting behavior
\end{itemize}

\textbf{Outliers:} 
No outlier detection was performed as the data consists of categorical voting records with a constrained domain (yes/no/unknown), where all values are valid.

\textbf{Encoding:} 
Categorical voting responses were converted to numerical format:
\begin{itemize}
    \item "y" (yes) $\rightarrow$ 1
    \item "n" (no) $\rightarrow$ 0
    \item "unknown" $\rightarrow$ NaN (preserved as missing)
\end{itemize}
The ID column was retained for test set prediction submission but excluded from training features.

\textbf{Scaling:} 
No feature scaling was applied. The binary (0/1) encoding already provides uniform scale across features.

\subsubsection{Amazon Reviews Dataset Preprocessing}

\textbf{Missing Values:} 
The dataset had no missing values after loading, so no extra work needed.

\textbf{Outliers:} 
No explicit outlier removal was performed. The high-dimensional nature (10,000 features) of text features makes outlier detection computationally prohibitive and potentially harmful, as rare features may be informative for classification.

\textbf{Encoding:} 
The Class column (target) contains author names as strings. No encoding was necessary for features as they were already numerical. The ID column was preserved for test set predictions.

\textbf{Scaling:} 
Feature scaling requirements depend on the classifier:
\begin{itemize}
    \item \textit{For KNN and SVM:} StandardScaler applied to normalize the high-dimensional feature space
    \item \textit{For Decision Trees:} Scaling not needed, as tree-based methods are invariant to feature scaling
    \item \textit{Justification:} Distance-based methods (KNN) and kernel methods (SVM) are highly sensitive to feature scales, especially in high-dimensional spaces where some features may dominate distance calculations
\end{itemize}

\subsubsection{Phishing Website Dataset Preprocessing}

\textbf{Missing Values:} 
The dataset had no missing values. All website characteristic features were complete, likely because they are deterministically extracted from URL analysis.

\textbf{Outliers:} 
No outlier treatment was performed. The features are categorical/ordinal with predefined value ranges, representing different website characteristics. All values are valid within their respective domains.

\textbf{Encoding:} 
All features were already numerically encoded in the ARFF format. We preserved this encoding as it maintains ordinal relationships in features (e.g., security certificate validity levels).

We created an additional derived feature \texttt{Result\_Label} mapping the numerical target to descriptive labels:
\begin{itemize}
    \item -1 $\rightarrow$ "Legitimate"
    \item 0 $\rightarrow$ "Suspicious"  
    \item 1 $\rightarrow$ "Phishing"
\end{itemize}
This derived feature was dropped before training but used for interpretability in reporting.

\textbf{Scaling:} 
No feature scaling was applied. The categorical/ordinal nature of features means their relative ordering is meaningful, and standardization could distort these relationships.

\subsubsection{Road Safety Dataset Preprocessing}

This dataset required the most extensive preprocessing due to its large size, mixed feature types, and missing data across multiple columns.

\textbf{Dimensionality Reduction:}
Due to computational constraints, we performed strategic column removal:
\begin{itemize}
    \item \textbf{High-cardinality identifiers:} Removed \texttt{Accident\_Index}, \texttt{LSOA\_of\_Accident\_Location} (25,000+ unique values), \texttt{Local\_Authority} columns (356+ unique districts)
    \item \textbf{Perfect predictors:} Removed \texttt{Age\_of\_Driver} as it would perfectly predict our target \texttt{Age\_Band\_of\_Driver}
    \item \textbf{Geographic coordinates:} Dropped \texttt{Location\_Easting\_OSGR}, \texttt{Location\_Northing\_OSGR}, \texttt{Longitude}, \texttt{Latitude} to reduce dimensionality while retaining other location features
    \item \textbf{High-missing columns:} Removed \texttt{2nd\_Road\_Class} (48\% missing), \texttt{Junction\_Control} (48\% missing), \texttt{Propulsion\_Code} (23\% missing), \texttt{Casualty\_IMD\_Decile} (19\% missing)
\end{itemize}

\textbf{Missing Values:}
In inital pre-processing functions, missing values were left as NaN, as the decision tree classifier can handle them natively. For the KNN and SVM classifiers, rows with missing values were dropped, as the classifiers do not natively support NaN. This likely led to a degradation of performance on this dataset, as the road\_safety dataset had lots of missing values.

\textbf{Feature Engineering:}
\begin{enumerate}
    \item \textbf{Date decomposition:} Extracted \texttt{Date\_Month}, \texttt{Date\_Day}, \texttt{Date\_Year} from the \texttt{Date} column (format: DD/MM/YYYY) to capture temporal patterns while avoiding date parsing issues
    \item \textbf{Time period encoding:} Converted \texttt{Time} strings to numerical \texttt{Time\_Period}:
    \begin{itemize}
        \item 0 = Night (22:00-05:59)
        \item 1 = Morning (06:00-11:59)
        \item 2 = Afternoon (12:00-17:59)
        \item 3 = Evening (18:00-21:59)
    \end{itemize}
    This transformation reduces dimensionality while capturing accident patterns related to traffic conditions and visibility.
    \item \textbf{Sex encoding:} Converted \texttt{Sex\_of\_Driver} from string representations ("1.0", "2.0", "3.0") to numeric values
\end{enumerate}

\textbf{Outliers:}
No outlier removal was performed. In accident data, extreme cases (e.g., unusual weather conditions, rare vehicle types) are valid observations that may be important for classification.

\textbf{Encoding:}
Most categorical features were already numerically encoded in the OpenML dataset format. We preserved these encodings.

\textbf{Scaling:}
Scaling was applied for the SVM classifier using StandardScaler to normalize numerical features. For the Decision Tree classifier, no scaling was applied, as tree-based methods are invariant to feature scaling. No feature scaling was applied for KNN.

\textbf{Dataset Subsampling:}
Due to the dataset's large size (363,243 samples) and computational limitations, some experiments used random subsampling (e.g., 10,000 samples) with a fixed random seed for reproducibility. This trade-off prioritized experimental feasibility while maintaining representative data distribution.


\subsection{Impact of Preprocessing on Performance}

To evaluate the impact of preprocessing, we compared classifier performance with and without key preprocessing steps, particularly scaling.

\subsubsection{Effect of Feature Scaling}

Feature scaling had varying impacts depending on the classifier type:

\textbf{Distance-based methods (KNN):}
\begin{itemize}
    \item Feature scaling had made a major impact on the reviews dataset performance. The accuracy improved from 0.03 to 0.13. It made the model better but not good.
    \item Feature scaling not applied for road safety dataset, so we will take this into account for model evaluation.
\end{itemize}

\textbf{Kernel methods (SVM):}

    Kernel scaling was essential for SVM since performance tanked drastically for the datasets.
    \begin{itemize}
        \item Voting: 65\% $\rightarrow$ 97.7\% (+50\%)
        \item Amazon: Failed $\rightarrow$ 58.0\% (essential)
        \item Road Safety: 15\% $\rightarrow$ 28.1\% (+87\%)
    \end{itemize}
    \textbf{Reason:} SVM kernel functions compute distances between points. Unscaled features with different ranges (e.g., 0-1 vs 0-10000) cause large-scale features to completely dominate, which makes the algorithm ignore smaller-scale features. Additionally, the RBF kernel $\exp(-\gamma \|\mathbf{x}_i - \mathbf{x}_j\|^2)$ becomes numerically unstable without scaling.
    \textbf{Solution:} StandardScaler (zero mean, unit variance) ensures equal feature contribution.
    \textbf{Practical lesson:} Always scale features for SVM - it's not optional.

\textbf{Tree-based methods (Decision Trees):}
\begin{itemize}
    \item No measurable performance difference with or without scaling, as expected.
\end{itemize}


%=============================================================================
\section{Classifiers}
\label{sec:classifiers}

% Describe the 3 chosen classifiers
% Must be from at least 2 different paradigms/types
% Explain characteristics (brief, don't repeat lecture material)
% Justify your choice


\subsection{Classifier 1: Decision Tree}
\label{subsec:classifier1}

\textbf{Type/Paradigm:} Decision Tree

\textbf{Key Characteristics:}
\begin{itemize}
    \item \textbf{Recursive partitioning:} Splits data using binary decision rules on features to create a tree structure, making decisions interpretable as a series of if-then-else conditions
    \item \textbf{Scale-invariant:} Does not require feature scaling or normalization, as splits are based on threshold comparisons that are invariant to monotonic transformations
    \item \textbf{Native missing value support:} Can handle NaN values directly by learning which branch to follow when a feature value is missing, avoiding the need for imputation
    \item \textbf{Non-linear decision boundaries:} Captures complex interactions between features through hierarchical splits, enabling modeling of non-linear relationships without kernel tricks
\end{itemize}

\textbf{Expected Strengths:} 
Decision trees excel with categorical/mixed data types, handle missing values naturally, and provide interpretable models through visualization. They perform well on small-to-medium datasets with clear feature interactions (e.g., Voting, Phishing) and require minimal preprocessing. Their scale-invariance makes them robust across different feature ranges without normalization.

\textbf{Expected Weaknesses:} 
Prone to overfitting on noisy or complex datasets, especially when trees grow too deep without pruning. Sensitive to small data variations (high variance), which can produce different tree structures from slight changes in training data. May struggle with high-dimensional sparse data (e.g., Amazon Reviews) where feature space is too large.
\subsection{Classifier 2: Support Vector Machine}
\label{subsec:classifier2}

\textbf{Type/Paradigm:} Kernel-based / Maximum Margin

\textbf{Key Characteristics:}
\begin{itemize}
    \item \textbf{Margin maximization:} Finds the optimal hyperplane that maximizes the distance (margin) between classes, providing strong generalization guarantees
    \item \textbf{Kernel flexibility:} Can model both linear and non-linear decision boundaries through kernel functions without explicitly transforming the feature space (kernel trick)
    \item \textbf{Sparse solution:} Uses only support vectors (subset of training data) for predictions, making it memory-efficient despite potentially large training sets
    \item \textbf{Scale-sensitive:} Requires feature scaling/normalization as distance computations in kernel functions are heavily influenced by feature magnitudes
\end{itemize}

\textbf{Expected Strengths:}
SVMs excel with high-dimensional data where the feature-to-sample ratio is large (e.g., Amazon Reviews with 10,000 features). Linear kernels avoid overfitting in sparse high-dimensional spaces, while RBF kernels can capture complex non-linear patterns in low-dimensional data (e.g., Phishing, Voting). Strong theoretical foundations provide good generalization even with limited training samples. Robust to outliers due to margin-based optimization focusing on boundary cases.

\textbf{Expected Weaknesses:}
Training time scales poorly with dataset size ($O(n^2)$ to $O(n^3)$), making it computationally expensive for large datasets like Road Safety (requires subsampling). RBF kernels suffer from the curse of dimensionality in very high-dimensional spaces, potentially performing worse than linear kernels. Sensitive to hyperparameter choices (C, gamma) and requires careful tuning. Multi-class classification is handled through one-vs-one or one-vs-rest strategies, increasing computational complexity with many classes.

\textbf{Selection reasoning:}
We selected SVM specifically to investigate the relationship between dataset dimensionality and kernel performance. SVMs offer a unique capability to switch between linear and non-linear decision boundaries through kernel selection, making them ideal for testing our hypothesis: high-dimensional sparse data (Amazon: 10,000 features) should favor linear kernels, while low-dimensional structured data (Phishing: 9 features) should benefit from non-linear RBF kernels. This kernel flexibility allows us to systematically evaluate how the curse of dimensionality affects non-linear methods. Additionally, SVMs provide theoretical guarantees through margin maximization and clear interpretability through support vectors, making them valuable for understanding classifier behavior across diverse datasets.
\subsection{Classifier 3: K-Nearest Neighbor}
\label{subsec:classifier3}

\textbf{Type/Paradigm:} NN, non-parametric supervised learning

\textbf{Key Characteristics:}
\begin{itemize}
    \item lazy loading
    \item always retains all training data to make predictions
    \item training data is stored as vectors
    \item when a new element comes, it will be classified as the mean of the k surrounding nearest neighbors
    \item basically a majority voting
\end{itemize}

\textbf{Expected Strengths:}
\begin{itemize}
    \item Very simple to implement
    \item Extremeley fast for small datasets.
    \item can classify quite good with categorical data (like voting)
\end{itemize}

\textbf{Expected Weaknesses:}
\begin{itemize}
    \item Very basic. Probably not very good performance
    \item Extremely slow for large datasets.
\end{itemize}


%=============================================================================

\section{Experimental Methodology}
\label{sec:methodology}

% Explain your experimental setup
% How you ensure fair comparison
% Performance measures and why you chose them
% Parameter settings explored
% Train/test split strategy

\subsection{Experimental Design}
Our experimental design follows a systematic approach to ensure valid and reproducible results:

\subsection{Ensuring Fair Comparison}
To ensure meaningful comparisons, we:
\begin{itemize}
    \item Used an 80/20 train-test split for all classifiers on each dataset
    \item Applied consistent preprocessing for each dataset
    \item Used the same random seed for reproducibility (within classifiers)
    \item Evaluated all classifiers using identical metrics
    \item Normalized/scaled data appropriately for classifiers that require it
\end{itemize}

\subsection{Performance Measures}

We selected multiple performance measures to comprehensively evaluate classifier effectiveness. For the Kaggle Datasets, where the test set was unlabeled, we relied on holdout validation metrics from the training set. For Road Safety and Phishing, we looked at accuracy of the model performance on the test split of the data.
Additionally, we used Precision, Recall, and F1-Score to capture class-specific performance, especially important for imbalanced datasets. 
We also include training time, in order to discuss the tradeoff between model training time and performance.

\subsection{Parameter Exploration}

For each classifier, we explored multiple parameter configurations:

\subsubsection{Decision Tree Parameters}
For the holdout method training, we experimented with a Cartesian product of the following parameter values:
\begin{itemize}
    \item \textbf{Holdout Percentage:} [0.2, 0.3] - to evaluate the impact of training set size on model performance
    \item \textbf{Max Depth:} [None, 10] - to control tree complexity and prevent overfitting
    \item \textbf{Min Samples Split:} [2, 5] - to ensure sufficient data at each node for reliable splits
\end{itemize}
For Cross Validation, we used 10- and 5-fold CV with the following parameter grid, and used the GridSearchCV method from sklearn to find the best parameters (selects best model parameters based on mean validation accuracy):
\begin{itemize}
    \item \textbf{Max Depth:} [3, 5, 7, 10, 15, 20, None]
    \item \textbf{Min Samples Split:} [2, 5, 10, 20, 50]
    \item \textbf{Min Samples Leaf:} [1, 2, 5, 10]
\end{itemize}

\subsubsection{SVM Parameters}

We systematically explored two kernel types with varying regularization to understand the dimensionality-kernel performance relationship:

\textbf{Kernel Functions Tested:}
\begin{itemize}
    \item \textbf{Linear Kernel:} $K(\mathbf{x}_i, \mathbf{x}_j) = \mathbf{x}_i^T \mathbf{x}_j$
    \begin{itemize}
        \item Expected to excel on high-dimensional data (Amazon: 10,000 features)
        \item Avoids overfitting by maintaining linear decision boundaries
    \end{itemize}
    \item \textbf{RBF (Radial Basis Function) Kernel:} $K(\mathbf{x}_i, \mathbf{x}_j) = \exp(-\gamma \|\mathbf{x}_i - \mathbf{x}_j\|^2)$
    \begin{itemize}
        \item Expected to excel on low-dimensional data (Phishing: 9 features, Voting: 16 features)
        \item Can capture non-linear decision boundaries
        \item Gamma set to 'scale': $\gamma = \frac{1}{n\_features \times X.var()}$ (auto-adapts to feature variance)
    \end{itemize}
\end{itemize}

\textbf{Regularization Parameter (C):}
\begin{itemize}
    \item Tested values: [0.1, 1.0, 10.0]
    \item Controls trade-off between margin maximization and training error minimization
    \item Low C (0.1): Larger margin, more misclassifications allowed (prefer generalization)
    \item High C (10.0): Smaller margin, fewer misclassifications (fit training data closely)
\end{itemize}

\textbf{Complete Parameter Grid:}
\begin{itemize}
    \item Linear kernel: C $\in$ \{0.1, 1.0, 10.0\} (3 configurations)
    \item RBF kernel: C $\in$ \{0.1, 1.0, 10.0\}, gamma='scale' (3 configurations)
    \item \textbf{Total: 6 configurations per dataset, 24 total experiments}
\end{itemize}

\textbf{Validation Strategy:}
\begin{itemize}
    \item 80-20 train-validation split with stratification (preserves class distribution)
    \item Random seed: 42 for reproducibility
    \item Separate test set for Kaggle datasets (Voting, Amazon)
    \item Best configuration selected based on validation accuracy
    \item Final model trained on full training set for Kaggle submissions
\end{itemize}

\textbf{Rationale for Parameter Selection:}
The parameter grid was designed to answer key research questions: (1) Does kernel choice depend on dimensionality? (2) How sensitive is SVM to regularization in different dimensional spaces? (3) Can we identify optimal C values across diverse datasets? By testing both linear and RBF kernels across three C values, we systematically evaluate the hypothesis that high-dimensional data requires linear kernels while low-dimensional data benefits from non-linear kernels.

\subsubsection{Classifier 3 Parameters} 
We tested multiple different parameters and came to the conclusion that these were the best ones for knn for each dataset. We ran multiple tests and then compared the results. These are the optimized parameters: 
\begin{itemize}
    \item \textbf{Dataset phishing:} $n\_neighbors$: 9, weights: distance, p: 1
    \item \textbf{Dataset reviews:} $n\_neighbors$: 1, weights: distance, p: 2
    \item \textbf{Dataset road\_safety:} $n\_neighbors$: 11, weights: distance, p: 1
    \item \textbf{Dataset voting:} $n\_neighbors$: 7, weights: distance, p: 2
\end{itemize}


%=============================================================================
\section{Results}
\label{sec:results}

% Present your experimental results
% Use tables and figures
% Results for each dataset-classifier combination
% Different parameter settings
% Be clear and organized

This section presents the experimental results for all dataset-classifier combinations. Results are organized by dataset, with comprehensive performance metrics for each classifier configuration.

\subsection{Dataset 1: Voting Records Results}

\subsubsection{Overall Performance Comparison}
\subsection{Decision Tree Results - Voting Dataset}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{assets/voting_decision_tree_results_summary_table.png}
    \caption{Decision Tree Results Summary - Voting Dataset}
    \label{fig:voting_dt_results}
\end{figure}
The table above shows results for the Decision Tree classifier. The decision tree was able to reach consistently high accuracies of 96\% or higher. We can also see that grid search CV gave slightly lower training accuracies, but better generalization to the test set. Training times were negligible for single holdout splits, but increased to several seconds for GridSearchCV due to multiple folds and parameter combinations. Performance was stable across the two fold counts. Curiously, the 10-fold CV took less time to train than the 5-fold CV, possibly due to implementation details in sklearn's parallelization, or else local concurrent processes on the machine used for training.
Decision Tree Parameter Sensitivity:
Adjusting Max Depth yielded nearly identical results, suggesting the data is easily separable with shallow trees. Increasing Min Samples Split from 2 to 5 slightly reduced training accuracy, but validation accuracy stayed nearly the same, indicating that stricter splitting rules help reduce overfitting without hurting generalization.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{assets/voting_classification_report.png}
    \caption{Decision Tree Classification Report - Voting Dataset}
    \label{fig:voting_dt_classification}
\end{figure}
The classification report above does not give much new information, but shows that the dataset and the prediction errors for decision tree were roughly equally split across classes.

\subsection{K Nearest Neighbors Results- Voting Dataset}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{assets/voting_knn_results_summary.png}
    \caption{Decision Tree Results Summary - Voting Dataset}
    \label{fig:voting_knn_results}
\end{figure}
The aim was to get the best parameters for train accuracy and validation accuracy. Validation accuracies are consistently around 0.97-1.00, showing strong performance. Some configurations (e.g., k=7, weights=uniform, p=1 or 2) even reach perfect validation accuracy. Models with weights=distance often have train\_acc=1.0000, suggesting they perfectly fit the training data — possibly a slight overfitting risk. It was a small dataset. So the inference times are negligible.



\subsubsection{SVM Results - Voting Dataset}

\begin{table}[H]
\centering
\caption{SVM Performance on Voting Dataset}
\label{tab:voting_svm_results}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Kernel} & \textbf{C} & \textbf{Accuracy} & \textbf{F1-Score} & \textbf{Train Time} \\
\midrule
Linear & 1.0 & \textbf{0.9773} & \textbf{0.9730} & <1ms \\
RBF & 1.0 & 0.9545 & 0.9500 & <1ms \\
\bottomrule
\end{tabular}
\end{table}

Best configuration: Linear (C=1.0) achieved 97.7\% accuracy. Both kernels performed excellently on this binary classification task with 16 features. Training was nearly instantaneous due to small dataset size (218 samples).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{voting/svm/voting_viz_1_kernel_comparison.png}
    \caption{SVM kernel comparison - both exceed 95\% on Voting dataset}
    \label{fig:voting_svm}
\end{figure}

\subsection{Dataset 2: Amazon Reviews Results}
\subsection{Decision Tree Results - Amazon Reviews Dataset}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{assets/reviews_decision_tree_results_summary_table.png}
    \caption{Decision Tree Results Summary - Amazon Reviews Dataset.}
    \label{fig:reviews_dt_results}
\end{figure}
For this dataset, the decision tree classifier struggled significantly. The best accuracy was achieved with 10-fold CV and reached an average of 40\% on the left-out fold. However, the kaggle results showed only 34\% accuracy on unseen data. This shows the weakness of decision trees on large, high-dimensional datasets. The model likely overfit to the training data, leading to poor generalization.
Decision Tree Parameter Sensitivity: Hyperparameters like min samples split and max depth do not drastically change performance. The model consistently underperforms regardless of configuration, indicating that the decision tree is not well-suited for this dataset's characteristics.

\subsection{K Nearest Neighbors Results - Amazon Reviews Dataset}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{assets/reviews_knn_results_summary.png}
    \caption{Decision Tree Results Summary - Voting Dataset}
    \label{fig:results_knn_results}
\end{figure}
Most models have a Train Acc = 1.0000, meaning they perfectly fit the training data. This is a classic sign of overfitting, especially when the validation accuracy is much lower. Validation accuracy values are very low (mostly between 0.03 and 0.12). This shows that although the models learn the training data perfectly, they fail to generalize to new data. The gap between training and validation accuracy is extremely large. Strong overfitting! Changing k (1, 3, 5, 7, 9) doesn’t significantly improve validation performance. Both uniform and distance weighting yield similar results. The 5-Fold CV results show similar patterns to the Holdout method. Small training time for a small dataset. Expected.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{reviews/knn/knn_scaled_n=1}
    \caption{Decision Tree Results Summary - Voting Dataset}
    \label{fig:results_knn_scaled_1}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{reviews/knn/knn_unscaled_n=10}
    \caption{Decision Tree Results Summary - Voting Dataset}
    \label{fig:results_knn_unscaled_10}
\end{figure}
The above two figures show a comparison between scaled,n=1 and unscaled,n=10. We see that the scaled one performs significantly better. Since we don't know the test data, I over layed the predictions (orange) and the train data (blue) as a histogram. They should match.


\subsubsection{SVM Results - Amazon Reviews Dataset}

\begin{table}[H]
\centering
\caption{SVM Performance on Amazon Reviews (10,000 Features)}
\label{tab:amazon_svm_results}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Kernel} & \textbf{C} & \textbf{Accuracy} & \textbf{F1-Score} & \textbf{Train Time} \\
\midrule
Linear & 1.0 & \textbf{0.5800} & 0.5744 & 0.72s \\
RBF & 1.0 & 0.0267 & 0.0013 & 0.74s \\
\midrule
\multicolumn{5}{l}{\textit{Random baseline: 2\% (50 classes)}} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Critical Finding - Curse of Dimensionality:} Linear kernel achieved 58.0\% accuracy (29× random baseline) while RBF completely failed at 2.7\%. In 10,000-dimensional space, RBF's non-linear boundaries overfit catastrophically, while Linear's simpler boundaries generalize effectively. This demonstrates that high-dimensional data requires linear kernels. Notably, Linear accuracy was insensitive to C parameter (all values yielded 58.0\%).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{reviews/svm/amazon_viz_1_kernel_comparison.png}
    \caption{Dramatic kernel comparison: Linear (58\%) vs RBF (2.7\%) showing curse of dimensionality}
    \label{fig:amazon_svm}
\end{figure}

\subsection{Dataset 3: Phishing Results}

\subsection{Decision Tree Results - Phishing Dataset}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{assets/phishing_decision_tree_results_summary_table.png}
    \caption{Decision Tree Results Summary - Phishing Dataset}
    \label{fig:phishing_dt_results}
\end{figure}
Decision Tree had average performance on the phishing dataset. Validation accuracy was consistently lower than training accuracy, but only by about 10\%. Similar to the reviews dataset, GridSearchCV provided better generalization than single holdout splits.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{assets/phishing_decision_tree_classification_report.png}
    \caption{Classification Report for Best Model - Phishing Dataset (Decision Tree).}
    \label{fig:phishing_dt_classification}
\end{figure}
The weakest class accuracy was 0 (Suspicious), which also had the lowest support, so the poor performance here is possibly due to class imbalance.

\subsubsection{SVM Results - Phishing Dataset}

\begin{table}[H]
\centering
\caption{SVM Performance on Phishing Dataset}
\label{tab:phishing_svm_results}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Kernel} & \textbf{C} & \textbf{Accuracy} & \textbf{F1-Score} & \textbf{Train Time} \\
\midrule
Linear & 1.0 & 0.8450 & 0.8428 & 9ms \\
RBF & 10.0 & \textbf{0.8931} & \textbf{0.8928} & 9ms \\
\bottomrule
\end{tabular}
\end{table}

Best configuration: RBF (C=10.0) achieved 89.3\% accuracy, outperforming Linear by 5\%. In contrast to Amazon's high-dimensional space, this low-dimensional dataset (9 features) allows RBF to effectively capture non-linear decision boundaries. Class imbalance (7.6\% minority) was handled well. Training remained fast (<10ms) despite 1,353 samples.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{phishing/svm/phishing_viz_1_kernel_comparison.png}
    \caption{RBF outperforms Linear in low-dimensional space (Phishing: 9 features)}
    \label{fig:phishing_svm}
\end{figure}

\subsection{K Nearest Neighbors Results - Phishing Dataset}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{assets/reviews_knn_results_summary.png}
    \caption{Decision Tree Results Summary - Voting Dataset}
    \label{fig:phishing_knn_results}
\end{figure}
Best model: Holdout (80/20) with params {n neighbors: 9, weights: distance, p: 1}; Best validation: F1m=0.9933 | Acc=0.9908; \newline
Both accuracy and F1-score on validation data are very high (~ 0.93–0.99). This indicates that the KNN model distinguishes phishing from legitimate sites extremely well. The best configurations reach Val Acc ~ 0.99 and Val F1m ~ 0.993, meaning very balanced precision and recall. Overall, KNN is a highly effective choice for this dataset.


\subsection{Dataset 4: Road Safety Results}

\subsection{Decision Tree Results - Road Safety}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{assets/road_safety_decision_tree_results_summary_table.png}
    \caption{Decision Tree Results Summary - Road Safety Dataset}
    \label{fig:road_safety_dt_results}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{assets/road_safety_decision_tree_classification_report.png}
    \caption{Classification Report for Best Model - Road Safety Dataset (Decision Tree)}
    \label{fig:road_safety_dt_classification}
\end{figure}
The decision tree results for this road safety dataset show poor generalization and significant overfitting. Setting the max depth to 10 did not hurt the test set accuracy, although it brought down the training accuracy by almost half. This performance on a highly imbalanced multi-class dataset shows further weakness in the decision tree approach on large, complex datasets.

\subsection{K Nearest Neighbors Results - Road Safety}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{assets/road_safety_knn_summary_1}
    \caption{K Nearest Neighbors Results Summary - Road Safety Dataset}
    \label{fig:road_safety_knn_results_1}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{assets/road_safety_knn_summary_2}
    \caption{Decision Tree Results Summary - Voting Dataset}
    \label{fig:road_safety_knn_results_2}
\end{figure}
Best model: 5-Fold CV with params {n neighbors: 11, weights: distance, p: 1}; Best validation: F1m=0.3588 | Acc=0.4095; \newline
The KNN results for this phishing dataset show poor generalization and significant overfitting. While the training accuracy and F1-scores are perfect (~1.0) for most configurations—especially when using distance weighting—the validation accuracy and F1-scores drop sharply to around 0.28–0.41, regardless of k, p, or weighting scheme. This large gap between training and validation performance indicates that the model memorizes the training data but fails to correctly classify unseen samples. Changing distance metrics (p=1 vs p=2) or increasing k does not meaningfully improve results.\newline
Note that it took forever. So, we pruned the dataset to 100.000 entries instead of 260.000! This pruning significantly affected the performance!!! Even pruned, it took multiple hours on an extremeley powerful PC!


\subsubsection{SVM Results - Road Safety Dataset}

\begin{table}[H]
\centering
\caption{SVM Performance on Road Safety (Subsampled to 10K)}
\label{tab:road_safety_svm_results}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Kernel} & \textbf{C} & \textbf{Accuracy} & \textbf{Train Time (s)} & \textbf{vs Baseline} \\
\midrule
Linear & 1.0 & 0.2615 & 4.82 & +187\% \\
RBF & 1.0 & \textbf{0.2810} & 1.95 & \textbf{+209\%} \\
\midrule
\multicolumn{5}{l}{\textit{Random baseline: 9.09\% (11 classes)}} \\
\bottomrule
\end{tabular}
\end{table}

Best configuration: RBF (C=1.0) achieved 28.1\%, representing a 3× improvement over random guessing (9.1\%). While this appears low, it demonstrates significant learning on this challenging 11-class problem with imbalanced classes (19.2\% to 5.6\% range). The dataset was subsampled from 363K to 10K samples for computational feasibility. Medium dimensionality (66 features) allowed RBF to maintain slight edge over Linear. Training time scaled dramatically with C: Linear C=10.0 required 45s vs 1.6s for C=0.1.


%=============================================================================
\section{Conclusion}

\subsection{Summary of Findings}
Our experiments across four diverse datasets revealed key insights into classifier performance relative to dataset characteristics. One rather significant takeaway was the difficulty of training our classifiers on the road safety dataset - 300k+ observations is simply too many for most basic classifiers to handle in a reasonable time frame.

On the Reviews dataset, KNN performed the best, SVM with linear kernel came second, and decision tree was last. This shows that high-dimensional sparse data favors distance-based methods and linear decision boundaries.

On the Voting dataset, all classifiers performed well, with the SVM classifier reaching 97\% accuracy. This indicates that low-dimensional structured data can be effectively modeled by various classifiers. Also, American politicians often vote in predictable patterns, enabling high accuracy on this dataset.

On the Phishing dataset, decision tree and SVM achieved relatively good performance on test data, about 89\% for both. The KNN model, on the other hand, overfit the training data to a high degree (100\% classification rate) and thus performed quite poorly on the test data - about 12.4\%. This shows that while KNN can excel on some datasets, it is prone to overfitting on smaller, imbalanced datasets.

For Road Safety dataset, Age Band of Driver was a difficult target attribute to predict. Decision trees reached about 50\% accuracy on the test data, KNN acheived a high of about 40\%, and SVM performed the worst with about 30\% accuracy. This shows that none of the classifiers were well-suited for this complex multi-class problem with imbalanced classes. It's possible that in real-life, driver age is influenced by many unobserved factors not captured in the dataset.

\subsection{KNN parameter analysis}
When setting the "weights" parameter, the computation time immerdiately multiplied. Normally, the calculation were done within a few seconds up to a minute. But when using other weights, we had to stop the calculation after 2 hours since I did not finish until then.

The most impact had the "\$n\_neighbors\$". It did not impact the model that much but still the most. It could shift the accuracy about +-0.10. The reason is that the lower the number, the less values are being compared with. So, there could form a local bubble, where only a certain class is present, when in reality there value should be a totally different class. On the other hand, when k=n, then it takes just the average.

\subsection{Lessons Learned}
Through this exercise, we learned
\begin{itemize}
    \item The importance of proper data preprocessing
    \item How dataset characteristics influence classifier selection
    \item The value of systematic experimentation and parameter tuning
    \item The necessity of using multiple evaluation metrics
\end{itemize}


\end{document}
