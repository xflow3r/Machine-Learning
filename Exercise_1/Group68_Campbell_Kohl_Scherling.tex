\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{float}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[margin=1in]{geometry}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue
}

% Code listing style (if needed)
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny,
    showstringspaces=false
}

\title{\textbf{Exercise 1: Classification\\
VU Machine Learning 2025W}}

\author{
    Group 68\\[0.3cm]
    Campbell Lucas (12536848)\\
    Kohl Leonhard (12047036)\\
    Scherling Christopher (12119060)
}

\date{\today}

\begin{document}

\maketitle
\thispagestyle{empty}
\newpage

\tableofcontents
\newpage

\setcounter{page}{1}

%=============================================================================
\section{Introduction}
\label{sec:introduction}

% Brief overview of the exercise objectives
% What you aim to investigate
% Structure of the report

This report presents our work on Exercise 1 of the Machine Learning course, focusing on classification tasks across multiple datasets using different algorithms. The main objectives are to experiment with various classifier-dataset combinations, analyze preprocessing strategies, evaluate performance using multiple metrics, and compare holdout and cross-validation approaches.


%=============================================================================
\section{Datasets}
\label{sec:datasets}

% Describe each of the 4 datasets
% 2 from Kaggle competition
% 2 from Exercise 0
% Justify your choice - explain diversity:
%   - number of samples (small vs large)
%   - number of dimensions (low vs high)
%   - number of classes (few vs many)
%   - preprocessing needs

We selected four diverse datasets to comprehensively evaluate classifier performance across different data characteristics:

\subsection{Dataset 1: [Voting Records]}
\label{subsec:dataset1}

\textbf{Source:} Kaggle Competition (Course-specific)

\textbf{Characteristics:}
\begin{itemize}
    \item Number of samples (train/test): 218 / 217
    \item Number of features: 16
    \item Number of classes: 2 (Democrat vs. Republican)
    \item Feature types: Categorical (voting records on various congressional bills)
    \item Class distribution: Imbalanced (58.3% Democrat / 41.7% Republican)
\end{itemize}

\textbf{Description:}
This dataset contains congressional voting records for U.S. House Representatives, with each feature representing a vote (yes/no/abstain) on specific legislation. The classification task is to predict party affiliation (Democrat or Republican) based on voting patterns. This represents a classic political science application where voting behavior serves as a strong indicator of party membership.


\textbf{Challenges:} 
This dataset had a small sample size of only 218 training samples which created a big risk of overfitting. It also had a slight class imbalance
which could lead to a prediction bias towards the majority class. Also missing votes needed appropriate handling.

\subsection{Dataset 2: [Amazon Product Reviews]}
\label{subsec:dataset2}

\textbf{Source:} Kaggle Competition (Course-specific)

\textbf{Characteristics:}
\begin{itemize}
    \item Number of samples (train/test): 750 / 750
    \item Number of features: 10,000
    \item Number of classes: 50 (product categories)
    \item Feature types: Numerical
    \item Class distribution: Balanced (~2\% per class)
\end{itemize}

\textbf{Description:}
This text classification dataset contains Amazon product reviews that have been preprocessed into a high-dimensional feature representation. 
Each review must be classified into one of 50 product categories. This represents a challenging multi-class text classification problem typical in e-commerce and natural language processing applications.


\textbf{Challenges:}
This dataset had a very high dimensionality which was challenging. It also had a relatively small sample size compared to the feature count



\subsection{Dataset 3: [Phishing Website Detection]}
\label{subsec:dataset3}

\textbf{Source:} Exercise 0

\textbf{Characteristics:}
\begin{itemize}
    \item Number of samples: 1,353
    \item Number of features: 9
    \item Number of classes: 3 (phishing status: -1, 0, 1)
    \item Feature types: Categorical (website characteristics)
    \item Class distribution: Imbalanced (51.9\% / 40.5\% / 7.6\%)
\end{itemize}

\textbf{Description:}
This dataset contains features describing website characteristics used to detect phishing attempts. 
Features include URL structure, domain age, SSL certificate status, and other security-related attributes. 
The three classes represent different levels of phishing risk or legitimacy. 

\textbf{Challenges:}
Since all features were categorical proper encoding was needed. We also had a slight class imbalance in the dataset (7.6\%).
Also the 3-class problem is a bit more complex compared to binary classification. There was also some collinearity present among the predictors.


\subsection{Dataset 4: [Road Safety Casualties]}
\label{subsec:dataset4}

\textbf{Source:} Exercise 0

\textbf{Characteristics:}
\begin{itemize}
    \item Number of samples: 363,243 (subsampled to 10,000 for computational feasibility)
    \item Number of features: 66 (59 categorical, 7 numerical)
    \item Number of classes: 11 (casualty severity levels)
    \item Feature types: Mixed (predominantly categorical)
    \item Class distribution: Imbalanced (largest class 19.2\%, smallest class 5.6\%)
\end{itemize}

\textbf{Description:}
This large-scale dataset from the UK Department for Transport contains detailed records of traffic accidents, including information about road conditions, weather, vehicle types, location characteristics, and casualty outcomes. 
The classification task is to predict casualty severity level (ranging from slight injuries to fatalities) based on accident circumstances. 
This represents a real-world public safety application where accurate predictions could inform emergency response resource allocation.

\textbf{Challenges:}
The dataset is massive with 363.000 samples which made it really challenging to compute. Some teammembers needed to use subsampling since we didn't have the needed compute power.
Also since 11 classes are present this creates very complex decision boundaries. There are also missing values across multiple features. 


\centering

%=============================================================================
\section{Data Preprocessing}
\label{sec:preprocessing}

% Describe preprocessing steps for each dataset
% Justify why you chose these steps
% Discuss impact of preprocessing (especially scaling) on results

This section describes the preprocessing steps applied to each dataset and justifies these choices based on data characteristics and classifier requirements.

\subsection{General Preprocessing Pipeline}

Our preprocessing pipeline follows these general steps:
\begin{enumerate}
    \item Data loading and initial exploration
    \item Handling missing values
    \item Outlier detection and treatment
    \item Feature encoding (for categorical variables)
    \item Feature scaling/normalization
    \item Feature selection (if applicable)
\end{enumerate}

\subsection{Dataset-Specific Preprocessing}

\subsubsection{Dataset 1 Preprocessing}

\textbf{Missing Values:} [Strategy used and justification]

\textbf{Outliers:} [Detection method and treatment]

\textbf{Encoding:} [e.g., One-Hot Encoding for categorical features]

\textbf{Scaling:} [e.g., StandardScaler, MinMaxScaler, or none]
\begin{itemize}
    \item \textit{Justification:} [Why this scaling method was chosen]
\end{itemize}

\subsubsection{Dataset 2 Preprocessing}

[Similar structure as Dataset 1]

\subsubsection{Dataset 3 Preprocessing}

[Similar structure]

\subsubsection{Dataset 4 Preprocessing}

[Similar structure]

\subsection{Impact of Preprocessing on Performance}

To evaluate the impact of preprocessing, we compared classifier performance with and without key preprocessing steps, particularly scaling.


%=============================================================================
\section{Classifiers}
\label{sec:classifiers}

% Describe the 3 chosen classifiers
% Must be from at least 2 different paradigms/types
% Explain characteristics (brief, don't repeat lecture material)
% Justify your choice

We selected three classifiers from different learning paradigms to ensure comprehensive evaluation:

\subsection{Classifier 1: [e.g., Random Forest]}
\label{subsec:classifier1}

\textbf{Type/Paradigm:} Ensemble Learning / Tree-based

\textbf{Key Characteristics:}
\begin{itemize}
    \item [List 3-5 key characteristics]
    \item [e.g., Handles non-linear relationships]
    \item [e.g., Robust to outliers]
    \item [e.g., Provides feature importance]
\end{itemize}

\textbf{Expected Strengths:} [What types of datasets/problems is it good for?]

\textbf{Expected Weaknesses:} [What are its limitations?]

\subsection{Classifier 2: [e.g., Support Vector Machine]}
\label{subsec:classifier2}

\textbf{Type/Paradigm:} Kernel-based / Maximum Margin

\textbf{Key Characteristics:}
\begin{itemize}
    \item [List key characteristics]
\end{itemize}

\textbf{Expected Strengths:} [Strengths]

\textbf{Expected Weaknesses:} [Weaknesses]

\subsection{Classifier 3: [e.g., Neural Network / Logistic Regression]}
\label{subsec:classifier3}

\textbf{Type/Paradigm:} [Different from above two]

\textbf{Key Characteristics:}
\begin{itemize}
    \item [List key characteristics]
\end{itemize}

\textbf{Expected Strengths:} [Strengths]

\textbf{Expected Weaknesses:} [Weaknesses]

\subsection{Classifier Selection Rationale}

We selected these three classifiers to ensure:
\begin{itemize}
    \item Diversity in learning paradigms (at least two different types)
    \item Coverage of different algorithmic approaches
    \item Ability to evaluate performance across various data characteristics
    \item [Additional justification]
\end{itemize}

%=============================================================================
\section{Experimental Methodology}
\label{sec:methodology}

% Explain your experimental setup
% How you ensure fair comparison
% Performance measures and why you chose them
% Parameter settings explored
% Train/test split strategy

\subsection{Experimental Design}

Our experimental design follows a systematic approach to ensure valid and reproducible results:



\subsection{Ensuring Fair Comparison}

To ensure meaningful comparisons, we:

\begin{itemize}
    \item Used the same train-test splits for all classifiers on each dataset
    \item Applied consistent preprocessing for each dataset
    \item Used the same random seed for reproducibility: [SEED\_VALUE]
    \item Evaluated all classifiers using identical metrics
    \item Normalized/scaled data appropriately for classifiers that require it
    \item [Additional measures taken]
\end{itemize}

\subsection{Performance Measures}


We selected multiple performance measures to comprehensively evaluate classifier effectiveness:



\subsubsection{Additional Metrics}
\begin{itemize}
    \item \textbf{Confusion Matrix:} For detailed error analysis
    \item \textbf{ROC-AUC:} [If applicable for binary/multi-class]
    \item \textbf{Training Time:} To evaluate efficiency
    \item \textbf{Prediction Time:} For real-world deployment considerations
\end{itemize}

\textit{Note:} For multi-class problems, we use [macro/weighted] averaging for precision, recall, and F1-score because [justification].

\subsection{Parameter Exploration}

For each classifier, we explored multiple parameter configurations:

\subsubsection{Classifier 1 Parameters}
\begin{itemize}
    \item \textbf{Parameter 1:} [Values tested and why]
    \item \textbf{Parameter 2:} [Values tested and why]
    \item \textbf{Parameter 3:} [Values tested and why]
\end{itemize}

\subsubsection{Classifier 2 Parameters}
[Similar structure]

\subsubsection{Classifier 3 Parameters}
[Similar structure]

\subsection{Computational Environment}

All experiments were conducted in the following environment:
\begin{itemize}
    \item \textbf{Hardware:} [CPU, RAM specifications]
    \item \textbf{Software:} Python 3.X, scikit-learn X.X.X, [other libraries]
    \item \textbf{Random Seed:} [Fixed seed value for reproducibility]
\end{itemize}

%=============================================================================
\section{Results}
\label{sec:results}

% Present your experimental results
% Use tables and figures
% Results for each dataset-classifier combination
% Different parameter settings
% Be clear and organized

This section presents the experimental results for all dataset-classifier combinations. Results are organized by dataset, with comprehensive performance metrics for each classifier configuration.

\subsection{Dataset 1 Results}

\subsubsection{Overall Performance Comparison}




\subsubsection{Parameter Sensitivity Analysis}



\subsubsection{Confusion Matrices}



\subsection{Dataset 2 Results}

[Similar structure as Dataset 1]

\subsection{Dataset 3 Results}

[Similar structure]

\subsection{Dataset 4 Results}

[Similar structure]

\subsection{Aggregated Results Overview}



%=============================================================================
\section{Analysis and Discussion}


% Analyze and discuss your results
% Compare classifiers across datasets
% Identify patterns and trends
% Discuss parameter sensitivity
% Explain preprocessing impact
% Discuss what worked and what didn't

\subsection{Classifier Performance Comparison}

\subsubsection{Overall Performance Trends}



\textbf{Key Findings:}
\begin{itemize}
    \item [Finding 1: e.g., "Classifier A consistently outperformed others on high-dimensional datasets"]
    \item [Finding 2: e.g., "Classifier B showed best performance on imbalanced datasets"]
    \item [Finding 3]
\end{itemize}

\subsubsection{Dataset-Specific Observations}

\textbf{Small vs. Large Datasets:}
[Discuss how classifiers performed differently based on dataset size]

\textbf{Low vs. High Dimensional Datasets:}
[Discuss impact of dimensionality]

\textbf{Few vs. Many Classes:}
[Discuss how number of classes affected performance]

\subsection{Parameter Sensitivity}

\subsubsection{Classifier A Sensitivity}



[Discuss which parameters had the most impact and why]

\subsubsection{Classifier B Sensitivity}

[Similar analysis]

\subsubsection{Classifier C Sensitivity}

[Similar analysis]

\subsection{Preprocessing Impact}

\subsubsection{Effect of Scaling}




\subsubsection{Effect of Other Preprocessing Steps}

[Discuss impact of outlier removal, missing value imputation, etc.]

\subsection{Patterns and Trends}

\subsubsection{Which Methods Work Well?}

[Identify classifiers that consistently performed well and explain why]

\subsubsection{Which Methods Did Not Work Well?}

[Identify classifiers that underperformed and explain potential reasons]

\subsubsection{Is There One Method Outperforming All Others?}

[Discuss whether any single classifier dominated across all datasets or if performance was dataset-dependent]

\subsection{Performance Improvements}

\textbf{Strategies that Improved Results:}
\begin{enumerate}
    \item [e.g., "Hyperparameter tuning improved Classifier A by XX\%"]
    \item [e.g., "Feature scaling was crucial for Classifier B"]
    \item [e.g., "Handling class imbalance through SMOTE/class weights"]
    \item [Additional strategies]
\end{enumerate}

\subsection{Efficiency Analysis}

\subsubsection{Training Time Comparison}


\subsubsection{Scalability with Dataset Size}



[Discuss how runtime scaled with dataset size for each classifier]

\subsection{Unexpected Findings and Issues}

\textbf{Unexpected Results:}
\begin{itemize}
    \item [Any surprising observations]
\end{itemize}

\textbf{Challenges Encountered:}
\begin{itemize}
    \item [Technical issues, convergence problems, etc.]
    \item [How you addressed them]
\end{itemize}

%=============================================================================
\section{Holdout vs. Cross-Validation Comparison}
\label{sec:holdout_cv}

% Compare holdout to cross-validation
% Pay attention to splits and settings
% Are there differences? Why? In which metrics?

To evaluate the robustness of our results, we compared holdout validation with k-fold cross-validation.

\subsection{Methodology}

\textbf{Holdout Validation:}
\begin{itemize}
    \item Train-test split: [e.g., 80-20]
    \item Random seed: [SEED]
    \item Single split used for all experiments
\end{itemize}

\textbf{Cross-Validation:}
\begin{itemize}
    \item Method: [e.g., 5-fold or 10-fold CV]
    \item Stratified: [Yes/No and why]
    \item Random seed: [SEED]
\end{itemize}

\subsection{Results Comparison}





\subsection{Analysis}

\subsubsection{Differences Observed}

[Discuss whether holdout and CV results differ significantly]

\subsubsection{Variance and Stability}

[Discuss standard deviation from CV - which classifiers are more stable?]

\subsubsection{Reasons for Differences}



\subsubsection{Which Metrics Are Most Affected?}

[Analyze if certain metrics (accuracy, precision, recall) show more variation between holdout and CV]

\subsection{Recommendations}




% Optional section: Discuss your Kaggle submissions
% What strategies worked best?
% How did you rank?

\subsection{Competition Performance}





\subsection{Lessons Learned from Competition}

[Discuss what worked well and what didn't in the competitive setting]

%=============================================================================
\section{Conclusion}
\label{sec:conclusion}

% Summarize your main findings
% Answer the key questions
% What did you learn?
% Future work suggestions


\subsection{Lessons Learned}

Through this exercise, we learned:
\begin{itemize}
    \item The importance of proper data preprocessing
    \item How dataset characteristics influence classifier selection
    \item The value of systematic experimentation and parameter tuning
    \item The necessity of using multiple evaluation metrics
    \item [Additional lessons]
\end{itemize}

\subsection{Future Work}

Potential directions for future investigation:
\begin{itemize}
    \item [e.g., "Exploring ensemble methods combining our classifiers"]
    \item [e.g., "Investigating more advanced feature engineering techniques"]
    \item [e.g., "Testing additional classifier types"]
    \item [Additional suggestions]
\end{itemize}

\subsection{Final Remarks}

[Concluding thoughts on the exercise and machine learning classification in general]

%=============================================================================
\section*{Group Work Organization}


\textbf{Collaboration Process:}
[Describe how you worked together, meetings held, communication methods, etc.]

\textbf{Equal Contribution:}
All group members contributed equally to this project through [describe how equal contribution was ensured].

%=============================================================================
% References (if needed)
% \bibliographystyle{plain}
% \bibliography{references}

\end{document}